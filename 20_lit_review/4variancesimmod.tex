\section{Variable Behaviour in Simulation \& Modelling}\label{sec:dynamism_in_sm}
% Dynamic / contingent behaviours in simulation and models
% IMO this is where a lot of stuff like variance in process mining, sanitisation
% of logs, log noise removal / injection goes.

In \cite{wallis2018caise}, PyDySoFu was used to model behaviour that changed as
the simulation progressed. Behaviour undergoing variance appears in literature
from many fields, but some themes stand out. Researchers are often interested
in:

\begin{itemize}
    \item Removing small variations from datasets in order to mine the original
    process (that real-world actors might be deviating from), referred to as
    sanitisation,
    \item Inserting variations so as to produce datasets with 
\end{itemize}

\subsection{Business Process Modelling \& variation in behaviour}
% process mining for processes which exhibit natural variations
\inline{Is this going from data to a model? Models to data? Potentially multiple models either way depending on their use? Consider this and possibly restructure.}

In real-world business processes, natural variation is difficult to avoid. This
is because business processes are inherently \sociotechnical, and so can be
expected to exhibit at least slight variations due to the mistakes of human
actors executing those processes. Variations can effectively take two forms:

\begin{enumerate}
    \item Some variations are expected, where predictable shifts in behaviour
    emerge over time. Examples would be habits forming which deviate from
    prescribed processes, skipped steps, or paths of a fork in a process
    becoming effectively ignored as others become the default (essentially
    producing redundancy in the model).
    \item Unexpected variations can occur if an actor behaves erratically,
    information is improperly recorded in a log (and so \emph{seems} to exhibit
    variance), or if some accident occurs. This appears as random noise in
    collected data, and is difficult to statically embed in any model, as change
    might take myriad forms and occur at an arbitrary number of points in the
    process.
\end{enumerate}

As these two forms of variance must be modelled differently, they are treated
differently in a business process model exhibiting variance. Typically the
second is treated as noise: undesired and a distraction from a model built to
reflect a prescribed process. They are therefore removed via sanitisation, and
are discussed in \cref{subsec:review_synthetic_datasets}. The first, variation
which might be reflective of a model as it can be expected to be \emph{executed}
--- even when this was not an intended or prescribed version --- might be
interesting to modellers. These situations might arise, for example, where
sociotechnical variance within the context of the broader system is the specific
subject of investigation. Degraded modes in these systems are a good example of
this~\cite{johnson2007degradedmodes}.


\labelledsubsec{Variations in Process Models}{bpm_variation}
Discussing 


\subsection{Process mining \& variation in data}
% working with noisy data: sanitisation / coping strategies
\inline{Write a short subsection on the trend of santising data / coping with noisy datasets. Sometimes behavioural variance isn't desirable.}
\inline{Consider making noisy data a subsubsection of \cref{subsec:review_synthetic_datasets}.}


\inline{So, there's some stuff to cite on sanitisation and mining in the
    presence of noise --- see On process model synthesis based on event logs
    with noise, \cite{Mitsyuk_2016}.}

    \inline{Add more stuff to be cited here, at the very least...}

Process Mining is a field which necessarily deals with erroneous data. As
processes are identified within event logs sourced from real systems,
inconsistencies in data collected or execution of a prescribed process results
in data fed to a mining algorithm which is at best not indicative of the desired
result, and at worst indicative of a different one altogether. As a result,
variation in process logs is a subject of active research in the community.

There are two main research efforts involving mining on log data with variance:

\begin{enumerate}
    \item Some researchers look to minimise the impact of log variance on the
    outcome of process mining. This can be done through the development of
    mining algorithms which are able to cope somewhat with variance. Many
    algorithms attempting to solve this problem have been developed, but their
    effectiveness depends on the kind of variance present and the degree to
    which those different variances are expressed in the
    data\cite{Mitsyuk_2016}.
    \item Other researchers look to identify noise in event logs before they are
    mined, processing them to eliminate any variance before mining begins. This
    requires classification of noise and the removal of suspect
    traces\cite{Cheng2015logsanitization}.
\end{enumerate}

Another perspective on the problem is that noise cannot be successfully
eliminated, but that training on empirical noise limits a researcher's control
over an experiment. The argument here is usually along the lines that empirical
noise is effectively impossible to predict, exert control over, or classify
entirely, so any testing of tools using that data is flawed. Without knowing an
algorithm's response to specific kinds of variance, a researcher can't compare
one approach properly or reproducibly against another. It is therefore important
to \emph{produce logs with controlled kinds of variance}, so as to create a kind
of synthetic workspace where algorithms are tested against synthetic data with
known kinds and degrees of variance. Once they are reproducibly tested against
known good datasets, they can undergo empirical verification by using data
captured ``in the wild''.

Naturally, similar approaches exist outside of process mining, as the
requirement for synthetic data is a common one. In potentially sensitive data
collected on the public --- census or health data for example --- there may be a
need to publish data which is at least partially
synthetic~\cite{little1993statistical,Drechsler_2011,rubin1993discussion}.
\citeauthor{Drechsler_2011} presents an array of simple statistical methods for
producing this~\cite{Drechsler_2011}. \citeauthor{koenecke2020syntheticgenecon}
note that, depending on the nature of the data \emph{needed} for a given
application, different methods are appropriate, meaning a variety of techniques
are required~\cite{koenecke2020syntheticgenecon}, and give an overview of
methods suitable in economics. MetaSim\cite{kar2019metasim} produces data using
probabilistic grammars for training neural nets in a manner naturally resilient
to variance by including an appropriate amount via the trained grammar, thereby
injecting a guaranteed correct degree of noise. Admittedly neural nets are a
common source of synthetic data in modern literature and a research subject with
a growing need for training data, perhaps best exemplified in the community's
production of another kind of neural net specifically for this purpose:
Generative Adversarial Nets, or GANs~\cite{goodfellow2014generative}.

Approaches specific to the generation of synthetic event logs are also
abundant\cite{stocker2013secsy, pourmasoumi2015business, Loreti_2019,
Yousfi_2015, ExecutableBPMNMitsyuk}, and as PyDySoFu's original use was in
\sociotechnical modelling, this is our primary interest. \inline{Shugurov paper
from 2014 should be checked for citing here too. There's a useful summary in On
Business Process Variants Generation (Pourmasoumi 2015).}



% \inline{OK! So now we need to go through the lit, in particular the five
% articles cited above. Some notes below too.}

In \cite{stocker2013secsy, stocker2014secsy} \citeauthor{stocker2013secsy}
describe a method for injecting variance into synthetic event logs (``traces'').
In \cite{stocker2013secsy}, a method is described whereby security-specific
alterations to traces can be made which represent the behaviour of an attacker
in some \sociotechnical system. Variance can be injected by statically
manipulating a process before simulating it to generate traces or making
modifications to traces after simulation. A supporting tool, ``Secsy'', is
provided in \cite{stocker2014secsy}.

A similar approach approach is provided in \cite{pourmasoumi2015business}, where
alterations are made to a process before simulation occurs. In terms of
alterations made to the model directly (and not produced traces), Secsy only
supports a limited number of operations on a model: transformation of
\texttt{AND} and \texttt{OR} gateways to the alternative kind, and swapping the
ordering of modelled activities. The method proposed in
\cite{pourmasoumi2015business} is able to make use of a much broader gamut of
alterations, by limiting themselves to mutating only block-structured processes
which they represent as ``structure trees''. Working with a tree-like structure
allows for edits to be made which preserve the model's validity, and a table of
ten potential --- reportedly non-exhaustive --- modifications to a model are
suggested, far more than suggested in the various works on Secsy.


The authors claim that the limitation of requiring block-structured models does
not impact the broad applicability of their approach, as \citeauthor{chenthesis}
claims that around 95\% of BPMN models can be represented this
way~\cite{chenthesis}. However, that claim should be held with some scepticism.
The citations for this claim are \cite{Thom2009ActivityPI} and a paper by
\citeauthor{polyvyanny2010structuring} which is most likely
\cite{polyvyanny2010structuring}\footnote{The citation indicates a paper
presented a year earlier than \cite{polyvyanny2010structuring}, and the author
has given talks with the same title and published other works with similar
titles --- although it is possible a paper with the same name was published a
year earlier, and this could change \citeauthor{chenthesis}'s claim, some
confidence can be had that this is a simple referencing error or typo.}. The
first work checks 214 process models against a set of patterns which are
specifically not formalised, and the second presents some formal work on the
translation of process models following the block structure relied on in
\cite{pourmasoumi2015business} and \cite{chenthesis}. However, the two works
never cite each other, the application of the formal translations to the
patterns presented is non-trivial, and no further explanation as to the
application required appears to be presented in the thesis. One could suppose
that the translation of the patterns to block-structuring could be automated by
an implementation of the theory presented in \cite{polyvyanny2010structuring}.
In any case, \citeauthor{chenthesis} notes in \cite{chenthesis} that the
requirement of block-structuring on a process model is a limiting factor in the
application of their own work in their conclusions, and so the broad
applicability claimed in \cite{pourmasoumi2015business} should be taken with
healthsome caution. After some exhaustive citation reading we can conclude that
neither approach supports effective production of synthetic event logs
exhibiting a wide gamut of variances by statically manipulating a BPMN model
prior to simulation, although the existence of both methods suggests it would be
a valuable research outcome.


% ===== better applied to synthetic data sans variance =====
% In \cite{ExecutableBPMNMitsyuk}, \citeauthor{ExecutableBPMNMitsyuk} provide a
% more recent example of an argument for synthetic traces pertaining to 

\inline{In \cite{Loreti_2019}, we can find good things I should write about.}


\inline{In \cite{Yousfi_2015}, we can find good things I should write about.
However after going through it it's clearly more variability in \emph{models},
not variability in \emph{data}. Belongs in another subsection.}


\begin{enumerate}
    \item \cite{Yousfi_2015} --- unread, v interesting
    \item \cite{stocker2013secsy} for secsy, and \cite{stocker2014secsy}, the associated
    tooling paper
    \item \cite{pourmasoumi2015business} generates synthetic logs with variance,
    just like secsy, but instead of making edits to the process before
    simulation using a ``structure tree'' representation and identifying points
    suitable for mutation.
    \item \cite{Loreti_2019} --- unread, v interesting
    \item \cite{ExecutableBPMNMitsyuk} --- Aalst generating logs from models. No
    variance but they make the case that synthetic data is needed by the
    community \emph{and} it's a big name taking a swing, too. Could combine well
    with \cite{pourmasoumi2015business} to get variance without actually
    producing new techniques, assuming a limitation of the sim approach to
    block-structured models (which I think they already impose anyway…)
    % \item \cite{kar2019metasim} where probabilistic grammars are used to generate training
    % data for neural nets. Not directly related but an example of how broad
    % synthetic generation is (and a good case for there being a broad requirement
    % in research and industry for synthetic datasets too). Their approach
    % naturally introduces plenty of noise.
\end{enumerate}
