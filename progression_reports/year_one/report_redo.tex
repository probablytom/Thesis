\documentclass[draft]{article}

\include{preamble}

\begin{document}

\maketitle

\begin{abstract}
  What am I trying to show?\todo{Actually write an abstract.}
\end{abstract}

% ==============================================================================
\section{Introduction}
\label{sec:introduction}
Systems\todo{should I be acknowledging OBASHI in this report?} modelling is a
complicated field, largely born of its highly interdisciplinary nature. This is
both a blessing and a curse.\par


On one hand, the breadth of the field gives a range of diverse problems to work
on, and many interesting opportunities spring forth as a result. At the same
time, this scope breeds a host of different philosophies regarding systems
modelling, often muddying the waters with regards the field's literature. this
can make identifying small-scale, incremental improvements on the existing
literature hard to identify. Also --- as will be discussed --- systems science
is notoriously difficult to verify with quantitative data.\footnote{As this
  point is pertinent to the research at hand, it will be discussed in
  \emph{SECTION}}\todo{identify section!}.\par

Fortunately, the Cambrian Explosion that is modern systems ressearch providees
an opportunity to do reearch which can make tangible, genione improvements to
everyday life. The potential here easily justifies the friction involved in
performing rigorous research in the field!\par

This work bgan in a somewhat aimless fashion, and arrived at exciting research
opportunities toward the end of this meandering. The structure of that journey
of discovery is reflected in the structure of this report: beginning without a
clear end in sight; splintering in different directions, as possibilities are
felt out; finally, converting on a well-founded conclusion by the end. This
end-result would be hard to achieve without the wandering involved, so this
report should be read with the patient optimism that things \emph{do} converge
by the end.\par

\todo{Write a summary of the chapters, once they're actually written.}




\part{Model Transformations}
% ==============================================================================
\section{Systems Modelling Literature}
\label{sec:literature}
The diversity of system modelling requirements results in a variety of
philosophies regarding what models should look like. A useful distinction
between these philosophies is their take on the traditional input, process,
output contept; different modelling paradigms \emph{capture} information in
different ways, allow varying degrees of anaysis to be performed on this
captured information, and lend themselves to different methods for the
\emph{exposition} of the results of this analysis.\par

The distinction matters. A person undertaking a modelling task must capture all
pertinent data --- many systems exhibit chaotic behaviour, so minute details
within captured information in the model can make a significant difference to
the modelling effort's end result --- persumably, the task is being carried out
so the modeller can learn something about the system in question! --- the kinds
of analysis that are possible are therefore of great importance, as available
analysis methods will have an impact on what can be inferred from the model ---
moreover, though, the modelling effort would be for nought if the recipient of
the outcome of the analysis found it illegible, meaning that the methods for
displaying the \emph{output} of that analysis is just as important as the other
factors!\par

While this may seem like a trivial observation, modelling paradigms in the wild
rarely succeed on all three counts, and many different approaches to these three
factors exist. UML, for example, is much more concerned with understanding the
structure of a model than performing a detailled analysis of that structure and
coming to quantitative, verifiable results. It's graphical nature belies the
fact that often, seeing the \emph{structure} of captured information is the most
valuable output of the modelling process in the paradigm. Even in mostly
graphical approaches, though, differences in philosophy can arise. To contrast:
these same systems are sometimes modelled using process calculi such as
\picalculus. Here, a satisfying visual representation is hard to find, but the
intent of the modelling effort is a detailled formal analysis. Difficulty in
capturing information, or in the legibility of the results, is a friction
sometimes justified by the detail of analysis a model permits. \par


With this in mind, comparing modelling approaches can help to explore both the
philosophies modellers enter into the practice of model construction with, and
the state of the art in actually constructing those models.\todo{Revisit this
  when writing about pdsf! We're targeting modellers who enter into the
  practice with a certain perspective, too. It's a nice callback to the related
  lit, and it limits our own scope in useful ways and gives us an ``out'', so
  we can justify some of pdsf's limitations.}\par


\subsection{A Note on Informal Modelling Paradigms}
\label{subsec:informal_model_lit}
Many\todo{consider deleting} informal modelling methods have appeared in an effort to fulfil
requirements in a few areas:

\begin{itemize}
\item Modelling hard-to-define systems\\
  Many systems are difficult to achieve low-resultion understandings of, because
  --- usually --- they are socio-technical in nature. The can sometimes have
  aspects\footnote{See Applied Systems Theory, in section \emph{FIND SECTION
      HERE}.}\todo{cref the applied systems theory section here.} of their
  systems captured, but the details required to produce a complete more are
  effectively infinite in scale.
\item Modelling black boxes\\
  It can be difficult-to-impossible to infer low-level detail from a system
  which is either obfuscated or intractable to break apart. Here, ``black
  boxes'' are characterised by their inputs and outputs where possible.
\item System scope\\
  Often, modelling is undertaken at a large scale. It is intractable to gather
  finely grained information in these situations for most modellers, leading to
  high-level modelling paradigms.
\end{itemize}

Systems with these properties are often assessed from a high level, and due to
their nature, are often either heterogeneous or partly human in nature (such as
business or military systems).\par

\subsection{UML, SysML \& OPM}
One approach to modelling these sorts of systems is the diagrammatic approach
taken by UML\cite{uml_citations}, which is the de-facto modelling framework for
software architectures. UML has been refined gradually over \~20 years of use,
and has grown to model a variety of systems. The variety of UML's diagramming
capabilities is well-summarised in a class diagram from its own
specifications:\par

\todo{include class diagram of UML diagram types, example from early spec found
  on wikipedia}

UML's origins are in mapping the structure of software, but it can now be used
to model a variety of activities via SysML\cite{sysml_citations}. A comparison
between this and its early competitor, OPM\cite{opm_citations}, which also
models a variety of systems, can be enlightening.\par

Where UML's expressiveness comes from an array of diagram types and extensions
to its ordinary diagrams --- such as SysML --- OPM uses a more curated set of
building blocks to build models from, gaining expressiveness with a simpler
modelling system. These building blocks are structured objects of information,
and processes that the information is transformed by --- concepts which prove to
be rather universal, as sen in OPM's cross-disciplinary
appeal\cite{opm_rna_research}. OPM's native focus on processes and simple
fundamental units delivers the useful feature that OPM models are generally
\emph{executable} for analysis, allowing deeper insights into the subject of
the model.\par

This simplicity is often cited as a benefit of
OPM: there is less for a modeller to learn, and the core concepts building the
framework are powerful. However, while both are recognised standards (UML is a
standard supported by the Object Management Group, where OPM is supported by
ISO), UML seems to see much wider industrial use. I\todo{can I use ``I'', or is
  that bad form in a progression report?} posit that this is actually
\emph{because} of the additional diagram classes in UML: each diagram class
becomes slightly simpler than an OPM diagram, and the result is that each
diagram comes with its own visual identity. This imparts lots of context about
what the model is displaying to a viewer at a glance, and reduces the amount of
friction involved in the modelling process.\par

The amount of context available at a glance turns out to be especially important
for UML, because UML can rarely be processed for analysis. Instead, results of
the UML modelling process are typically done by a visual analysis of a diagram.
Some tools exist for analysing he contents of a model\cite{eclipse_papyrus},
but these are typically limited in scope and too clunky to use --- often, this
friction in use overwhelms the potential gains from the tool.\par

UML is capable of dealing with a variety of socio-technical, hard-to-define
systems via its different diagram types and can work at a variety of resolutions
of detail, making it adept at handling ``black boxes''. However, as scale
increases, so does the number of diagrams --- UML therefore relies on the
quality of tooling available to cope with the difficulties of visual analysis at
scale.\par

% ===== OBASHI
\subsection{OBASHI}
\label{subsec:obashi}
A modelling platform which takes pains to fix this problem of visual analysis at
scale is OBASHI\cite{obashi_methodology}. OBASHI sees wide industrial use for
the specific task of modelling dataflow in socio-technical systems. Like UML,
OBASHI's modelling paradigm is entirely graphical. Models have simple
fundamental components --- ``elements'' and six kinds of ``relationships'' ---
which are composed together according to a series of rules. Elements are
separated into ``layers'', and an ordering is imposed which groups social and
technical kinds of elements separately, mediating dataflow between them via
business processes.\par

The modelling system is proprietary, and a sophisticated tool is available from
the company developing the methodology. OBASHI's limited scope, focusing on
dataflow, means that this tool can perform some automated analysis on an
otherwise graphical system model, such as impact analysis of element failure or
the generation of different diagrams representing different potential pathways
for dataflow between elements.\par

OBASHI takes an interesting perspective on the difficulties of modelling
hard-to-define systems, as the design of its diagrams is specifically
constructed to cater to non-technical recipients of diagrams. As the outputs of
analysis are often more diagrams, the diagrams that are produced are laid out in
such a way that stakeholders in a diagram who are unfamiliar with the nuances of
their subject can still understand the implications of an analysis. This is done
by utilising some of OBASHI's rules (specifically, element and relationship
persistence) to safely construct subgraphs which represent meaningful analyses.
OBASHI's ability to safely construct subgraphs eliminates the diagram scale
issues that OPM and UML/SysML have. OBASHI also caters to black-box scenarios
and socio-technical, hard-to-define systems by use of its rules. this is
possible, however, because OBASHI is capable of modelling only systems which
permit the flow of information or data --- this lmitation in scope allows OBASHI
to successfully address many of the difficulties of informal modelling, at the
cost of flexibility.\par

% ===== BPMN
\subsection{BPMN \& BPEL}
\label{subsec:bpmn}
A popular alternative for business modelling is the informal modelling solutions
found in BPMN\cite{bpmn_sources}, which has seen success in modelling business
processes. BPMN is generally a graphical modelling framework, with a language to
support executable process definitions via its companion language,
BPEL\cite{BPEL_SOURCES}. Not unlike OBASHI, BPMN aims to demonstrate concepts
graphically in a simple enough format that non-technical recipients of diagrams
can understand their meaning, while supporting more complex analyses where
necessary. BPMN can be considered the business equivalent of UML, and is also
supported by the Object Management Group\cite{source_for_this_maybe}.\par

BPMN has had unusual academic interest, in that some low-level concepts have
been applied to this otherwise very high-level notation. For example, BPMN
models can be transformed to DEVS models\cite{bazoun2014business}, a timing
model for agent-based systems built on an extension of Moore machines, a kind of
finite-state machine\cite{DEVS}.\todo{Write a little more on BPMN and how it
  relates to other modelling approaches, as well as what we can do with it.}\par


% ===== Applied Systems Theory
\subsection{Applied Systems Theory}



\todo{Anything else worth mentioning here? SSM? VSM?}





% \subsection{Formal Modelling Paradigms}
% \label{subsec:formal_model_lit}

% ===== Bigraphs
\subsection{Bigraphs}
\label{subsubsec:bigraphs}
A convenient segue from informal modelling techniques to formal ones is
Bigraphs, in that, to a degree, Bigraphs support both formal \emph{and} informal
paradigms. This is because Bigraphs are defined in three ways:

\begin{enumerate}
\item As a graphical notation
\item As an algebra
\item As a category
\end{enumerate}

As a result of a carefully constructed mathematical definition, Bigraphs have
the capacity to cope with often complicated features, such as safely composing
systems together, or the formal manipulation of a system's structure. This
mathematical foundation can make bigraphs complex to use for a non-technical
modeller. Fortunately, the graphical notation for birgaphs is both simple and
expressive.\par

Bigraphs are often discussed as having both \emph{space} and
\emph{motion}\cite{milner2009space}. The \emph{space} of a system is defined by
Milner (the original designer of the bigraph formalism) as the placements of
elements relative to each other --- this is represented, mathematically, as a
forest of nodes, where parental relationships in a tree in the forest represents
containment of one node inside another.\footnote{This definition is purely
  mathematical, and what that containment represents in the real world is the
  choice of the modeller.}\par

The \emph{motion} of a model is defined as how the model changes over time.
Milner represents system behaviour as change according to ``reaction rules'':
patterns which match on a subgraph of a complete bigraph, and dictate how the
matching subgraph looks in a future iteration of the bigraph. Reaction rules
turn out to be rather powerful, and can represent any system within a category
of systems called a \emph{bigraphical reactive
  system}\cite{milner_early_brs_definition} (BRSes). BRSes can represent a
number of calculi, including \picalculus and the ambient
calculus\cite{bigraphs_and_transitions_milner_jensen}.\par

While literature on the
subject is elusive, I\todo{Can I use ``I'' in a report?} infer that BRSes might
be represented as a temporal hypergraph, where a regular bigraph is represented
by a hypergraph, and the application of reaction rules modifies the hypergraph
as an atomic change at each unit of time.\todo{Consider deleting.}\par

Bigraphs seem to see little industrial success, but have inspired a number of
variants\cite{bigraphs_with_sharing,directed_bigraphs} and plenty of academic
interest\cite{impalas_stevegnani,bigraph_model_checking,bigraph_languages}.
Emergence has been studied in the context of Bigraphical Reactive Systems\cite{bigraph_emergence}, but
this is not one of the field's main active research areas at present.


% ===== Petri Nets
\subsection{Petri Nets}
\label{subsubsec:petrinets}
As an alternative to bigraphs, system modelling can be done formally via methods
such as Petri Nets\cite{petri_net_seminal}. A Petri Net is a directed graph of
states and processes those states can undergo to reach future states. Petri Nets
were born initially out of chemistry literature\cite{petri_net_seminal}, but
have seen applications in areas as diverse as workflow
modelling\cite{petri_net_workflow_modelling},
concurrency\cite{petri_net_concurrency}, and molecular networks in systems
biology\cite{petri_nets_for_biology}.\par

Petri nets have similarities with OPM, as they are both fundamentally concerned
with state (arbitrary state in the case of Petri Nets, and a more specific state
of structured data in the case of OPM) and processes that their states go
through. Unsurprisingly, some literature exists on the combination of petri nets
and OPM, which could eventually lead to a high-level modelling system with a
powerful mathematical framework underlying it.\par

\todo{Feels like I can write more here, but frankly, I'm not sure exactly what.
  Petri nets always seemed a little boring to me...}



% ===== CAS modelling via pdes, cellular automata...
\subsection{Complex Adaptive Systems Modelling}
\todo{write up cas modelling via cellular automata, pdes, etc}



% ===== Models in code: LogoNet, bespoke models like jagora
\subsection{Code and Bespoke Models}
\todo{Write up models in code, logonet being the big example but smaller ones
  too like PyCX., Jagora is an example of a bespoke model of a problem domain,
  which is worth covering because the practice of making those is pertinent to
  what we're getting at later on.}


\subsection{PyDySoFu}
\todo{Write up our own tool}



\subsection{Findings after Evaluating the Literature}
% Why we didn't do model transformation, in the end.
% However, PydySoFu turns out to be \emmph{quite} promising!


\part{Exploratory Development}
% ==============================================================================
\section{PyDySoFu's initial state}
\label{sec:pydysofu}
PyDySoFu's original version was an 80-line Python decorator for socio-technical
modelling, called \emph{fuzzi moss}\cite{4th_year_dissertation}. Since this
initial development, PyDySoFu had gone through a series of improvements,
culminating in a separation of the core dynamic fuzzing capacities from the
socio-technical components of the tool. Rather than a tool for dynamic fuzzing,
Fuzzi Moss had become a suite of fuzzers for socio-technical simulations built
on top of PyDySoFu.\par

\todo{Write a little about the technical underpinnings, where we rewrite
  \_\_getattribute\_\_.}

Some types of socio-technical behaviour, however, were difficult if not
impossible to implement via PyDySoFu's method for fuzzing. Specifically,
\emph{habit formation} and \emph{complex conditional fuzzing} were difficult if
not impossible to implement via PyDySoFu's approach, because:

\begin{itemize}
\item It wasn't possible to specify, via one of PyDySoFu's aspects, future
  fuzzing based on the previous return value of a target.\\
  This is important, because we often fuzz a workflow in real-life
  socio-technical systems when we know what the outcome of a previous change is.
  For example, after a workflow change leads to an unsafe situation in a safety
  critical system such as a rail network, the agent concerned is likely to amend
  their behaviour in future.
\item It wasn't possible to develop fuzzers which had a native concept of
  \emph{state} over time. That is: fuzzers could be considered as graph
  transforms, which were completely unaware of previous variants developed.
  Implementing this would make solving the issue above easier also. This was
  important, because there are scenarios where adaptation in a socio-technical
  system is unlikely to occur for a second time if it's already been tried, or
  if the adaptation led to poor results in the past.
\end{itemize}

Both issues were resolved by separating PyDySoFu's fuzzing capabilities into a
package of its own, and solving these problems on the level of \emph{aspect
  orientation} itself. Rather than iterating on PydySoFu, PyDySoFu was built as
a a collection of ``fuzzing aspects'' implemented in a framework which
splintered from PyDySoFu's codebase, called ``ASP''.\par


\section{ASP}
ASP\cite{asp_repo} is an aspect orientation framework written in python,
exploiting the technique which permitted PyDySoFu to operate in the first
place.\par

Where PyDySoFu injected functionality into an overridden
\texttt{\_\_getattribute\_\_} definition, ASP does the same --- but where
functionality was represented as a function which transformed an AST, ASP fuzzes
using objects. Where objects have special methods defined, ASP injects
functionality according to those methods. For example:

\begin{description}

\item[prelude(attribute, context, *args, **kwargs)] \hfill \\
  A \texttt{prelude} method on an object passed in as a fuzzer will run
  \emph{before} the target function is run. It has access to the target
  attribute, and to the original object the target is tied to in a
  \texttt{context}. It can also inspect the arguments the target function will
  be given.

\item[encore(attribute, context, result)]\hfill \\
  An \texttt{encore} method on an object passed in as a fuzzer will run
  \emph{after} the target function is run. It has access to the target
  attribute, and to the original object the target is tied to in a
  \texttt{context}, as well as the result of the target..

\item[around(attribute, context, *args, **kwargs)]\hfill \\
  An \texttt{around} method on an object passed in as a fuzzer wraps the target
  attribute in functionality which comes before and after it is executed. This
  can be used as an alternative to \texttt{encore} and/or
  \texttt{prelude}.\\
  An \texttt{around} method gets the same signature as the \texttt{prelude},
  because it must have at least the same functionality as a \texttt{prelude}
  method, but does not require the result of execution passed in as \emph{around
    is expected to execute the target attribute, with the arguments provided.}\\
  Worth noting is that if \texttt{around} exists at the same time as one or more
  of \texttt{prelude} or \texttt{encore}, then \texttt{prelude} or
  \texttt{encore} will be wrapped around the \texttt{around} method.

\item[error\_handling(attribute, context, exception)]\hfill \\
  An \texttt{error\_handling} method on an object passed in as a fuzzer will
  catch exceptions raised by the target attribute and attempt to handle them
  elegantly. This is a common piece of functionality in other aspect orientation
  frameworks\cite{aspect_j}.

\end{description}

In implementing these pieces of functionality, the foundation for PyDySoFu
became a fully-fledged aspect orientation library capable of capturing return
values of fuzzed functions, and storing information pertinent to the fuzzing
process --- results, variants, and anything else pertinent to the fuzzing case
--- in the fuzzer's internal state, because fuzzers can now be objects
satisfying the requirements for ASP to use them as aspects.\par

The work remaining at this stage is to rewrite PyDySoFu's fuzzing procedure to
be built on top of ASP.\par



% ==============================================================================
\section{Improvements to PyDySoFu}
\label{sec:pdsf_improvements}
As the above work satisfied the requirements for more complex fuzzing to be
built using PyDySoFu, the stage had been set to rewrite the library's core
functionality on top of ASP.\par

The procedure for doing this turned out to be relatively painless, thanks to
ASP's implementation being drawn from PyDySoFu's original mechanisms. Using
features already existing from earlier releases of PyDySoFu, such as the
application of a fuzzer to a target via a \texttt{fuzz\_function} function.
Implementation of the previous functionality in PyDySoFu turned out to be the
implementation of a single class:


\begin{figure}
  \begin{lstlisting}
class FuzzingAspect(IdentityAspect):

    def __init__(self, fuzzing_advice):
        self.fuzzing_advice = fuzzing_advice

    def prelude(self, attribute, context, *args, **kwargs):
        self.apply_fuzzing(attribute, context)

    def apply_fuzzing(self, attribute, context):
        # Ensure that advice key is unbound method for instance methods.
        if inspect.ismethod(attribute):
            reference_function = attribute.im_func
            advice_key = getattr(attribute.im_class, attribute.func_name)
        else:
            reference_function = attribute
            advice_key = reference_function

        fuzzer = self.fuzzing_advice.get(advice_key, identity)
        fuzz_function(reference_function, fuzzer, context)
  \end{lstlisting}
  \label{fig:fuzzing_aspect_code}
  \caption{The implementation of a basic fuzzing aspect}
\end{figure}


A benefit of ASP's object-oriented approach is that additional functionality can
be implemented by simple subclassing of the original FuzzingAspect, where a call
to \texttt{self.apply\_fuzzing} can apply actual fuzzing functionality, leaving
the subclass concerned only with additional features.\par

New fuzzers, implementing more complex behaviours, were then implemented and
tested. Specifially, the \texttt{IncrementalImprover} fuzzer class generates
\emph{multiple} variants of a target, and fuzzes in a manner similar to the
above\footnote{As the code for \texttt{IncrementalImprover} is long and complex,
  it has been omitted here, but can be found in the project
  repository\cite{pydysofu}.}. However, \texttt{IncrementalImprover} makes use
of ASP's introduction of catching return values to record the success of
different variants, and to generate future variants by applying fuzzing to
successful previous variants.\par

This additional funtionality is especially important for more complex
socio-technical simulations. For example: PyDySoFu's original functionality
permitted the same function to be fuzzed again and again with random results
each time, but fuzzing could not be stacked across multiple invocations of a
workflow step, i.e. introducing variance once, and then introducing
further variance where previous variance \emph{persisted through time}.\par

The \texttt{IncrementalImprover} is therefore a proof-of-concept for more
sophisticated simulations of socio-technical variance, and --- while the
functionality has not yet been introduced into Fuzzi Moss, this functionality is
intended to permit simulations specifically around instances of variance such as
habit formation.\todo{Anything else to say here?}\par


\section{Segue: Genetic Programming}
% ReaLX submission stuff
The implementation of incremental improvement rested on the ability to track
previous variants, and to record which variants were more ``successful'' than
others. It is clear to see that there is a similarity between the requirements
of the \texttt{IncrementalImprover} and that of genetic programming, then, as
genetic programming approaches to solving problems must construct many different
representations of processes and compare them for ``fitness''.\par

Genetic Programming approaches are often used to improve the state of an
existing codebase\cite{genetic_metaprogramming}\footnote{No term for this
  specific field was found. It will be referred to here as ``genetic
  metaprogramming''.}. Interestingly, genetic metaprogramming has been performed
similarly to how PyDySoFu operates already, by the modification of an
AST\cite{locoGP}, but --- like much of genetic metaprogramming literature ---
the technique is used to improve a codebase. An alternative approach presents
itself as a variant of what the \texttt{IncrementalImprover} was originally
intended to do: generate many variants of a single function, check its
``fitness'' against some success metric, and continue to fuzz successful
variants in future generations. This approach takes the typical AST-style
genetic modification, and employs it to attempt to solve regular genetic
programming problems.\par

Symbolic regression, a canonical genetic programming problem, was attempted then
using an alternative technique to traditional genetic programming angles, such
as constructing a tree representing a solution to a
problem\cite{koza_or_maybe_langdon}, or constructing more sophisticated directed
graphs which can converge on solutions faster\cite{cartesian_gp}. These
techniques inherently attempt to solve problems in a somewhat functional style.
Dynamic fuzzing, however, allows an imperative-style solution to a problem to be
engineered, where things like state can be manipulated by the solution via the
AST. Some variants of genetic programming, such as stack-based genetic
programming, also attempt this\cite{stack_based_gp} but ultimately rely on the
same fundamental functional style of other approaches.\par

To implement Genetic Programming in PyDySoFu, once the
\texttt{IncrementalImprover} had been implemented, did not require a significant
amount of change to the library. Changes in ASP and PyDySoFu to permit fuzzers
which could record a history of their fuzzing were sufficient to implement
a \texttt{GeneticImprover}, by subclassing the \texttt{IncrementalImprover}. As
new rounds are generated in the \texttt{GeneticImprover}, successful previous
variants are not just kept, but their ASTs \emph{spliced together} to mimic the
recombination found when comparing tree solutions in ordinary
genetic programming\cite{gp_seminal}. A symbolic regression was implemented
using this to confirm that it worked as intended, the source of which can be
found in the PyDySoFu repository\cite{pydysofu_repo}, and a paper outlining the
work in more detail found at \cite{realx_paper}.




\part{Resilience}

\section{Direction Change}
Evidently, PyDySoFu's core concept --- dynamic fuzzing --- is applicable to many
different fields, and poses a variety of research opportunities:

\begin{itemize}
\item Use for introducing variance socio-technical simulations\cite{honours_thesis}
\item Implementation of Genetic Programming
\item Design of aspect orientation frameworks capable of modifying their targets
  directly, rather than wrapping them, as is the case with traditional AOP
  techniques\cite{aspectj}.
\item Use for simulating changing behaviour in a complex adaptive system
\end{itemize}

A choice of direction therefore had to be made. As a library of socio-technical
fuzzers already exists\cite{fuzzi_moss}, and some preliminary work had been done
in simulating software development methodologies exposed to socio-technical
stress, this seemed an appropriate path to continue along. This was influenced
in part by the fact that, in the interim period between the initial work on
PyDySoFu\cite{honours_thesis} and the beginning of this study, a more
in-depth simulation of development methodologies under socio-technical stress
had been developed. Earlier this year, a paper on the work was
published\cite{caise_forum_18}.\par

Feedback from the submission of that paper had outlined some core issues which
had to be addressed. For one, validating the work against verifiable industry
data was difficult: some process logs for industry exist in public, but they are
uncommon. It can therefore be difficult to verify the problem domain's validity
\emph{before} verification. This is discussed more in \cref{sec:risks}. In
addition, the fuzzers used in \cite{caise_forum_18} were called into question,
as they weren't validated as being representative of the behaviour being
simulated.\par

Further work building on these simulations did present itself, however.
Particularly, more sophisticated variance introduced to the models which made
use of the new technologies in PyDySoFu --- particularly the potential to model
habit formation --- present themselves as future work building on the
already-published material. To perform this research properly would require a
degree of rigour and detail which was not present in the initial study of
variance as a cross-cutting concern.\par

Further work on the topic could be, then, to develop sophisticated models of
socio-technical systems, and verify these simulations for use in later
experimentation. These could be verified by experts in the domain. The same
could be done for more advanced fuzzers, working with researchers in Psychology
to ensure that the variance represented by the fuzzer was indicative of the
variance seen in the real world. The result would be an accurate and validated
model of socio-technical variance, which could prove otherwise elusive.\par

This foundation then paves the way for more complex experimentation. Emergent
phenomena could be spotted in socio-technical systems under stress that might not
be spotted without accurate simulation of that stress. For example:
safety-critical system literature makes reference to the degraded modes that
systems enter when agents make simplifications to a procedure which, while
innocuous on their own, interact to cause parts of a system to fail, often
leading to unsafe systems. Degraded modes are a mechanism which can preserve
vital system components until --- ideally --- failing components are put
right. Once a verified model of a system under variance is developed, then,
sophisticated assessments of the system's \emph{resilience} under
socio-technical stress can be simulated.\par

This technique could prove useful in a pragmatic sense, in industry: systems put
in place in healthcare, public infrastructure, business systems such as supply
chains, communication networks, and a host of similar real-world systems exhibit
variance, but actively testing systems under different kinds of variance can be
elusive as variance is difficult to model without treating it as a cross-cutting
concern\cite{caise_forum_18}. Performing a monte-carlo simulation on a
simulation under different kinds of stress, or the same stress under different
initial conditions, or the same stress \emph{and} initial conditions with
differing random seeds to represent real-world unpredictability, could allow a
proposed system to be probed for flaws by assessing how well it holds up in
real-world scenarios. Where socio-technical variance can be modelled as a
cross-cutting concern separated from any given domain model, a library of
variances can be composed, and then applied to a problem domain to probe its
resilience under sociotechnical stress by plugging a problem domain into a suite
of tests. While the motivation for the research \emph{could} have simply been
the application of new technologies in PyDySoFu, then, it is instead motivated
by this combined with a lack of these analyses in existing literature, and an
opportunity to marry this research with industry requirements.\par



% ==============================================================================
\section{Resilience and its Literature}
\label{sec:resilience}
\todo{read absolutely any literature on resilience other than the couple of
  Hollnagel papers I've got}



% ==============================================================================
\section{Thesis Statement}
\label{sec:thsis_statement}
Resilience as an emergent property of a socio-technical system can be difficult
to reason about quantitatively, as gathering information around a system's
resilience to stress typically means either constructing complex models
including variance and uncertainty in the problem domain, or exposure of a
prototype system to real-world scenarios, both of which are impractical to build
and intractable to validate.\par

Dynamic fuzzing offers a solution. Introducing variance as a cross-cutting
concern reduces the complexity of resulting system models, permitting simpler
problem domains and implementations of variance. The simplicity of each
component permits tractable model validation in turn. Validated models can then
be experimented on, and the model's resilience to socio-technical stress
quantified. Where variance is modelled in non-domain-specific ways, suites of
tests can be built for socio-technical systems where verified domain models can
be exposed to a simulation of real-world variance, and their response to that
variance realistically measured or asserted.\par



% ==============================================================================
\section{Future Plan}
\label{sec:future_plan}








% ==============================================================================
\section{Risks and Mitigating Actions}
\label{sec:risks}


Even when process logs are found, socio-technical systems often exhibit chaotic
behaviour, and a process log of one development system might not be
representative of how another system behaves. It can therefore be difficult to
verify the problem domain's validity \emph{before} verification.

System science is hard and repeatable experimental design with software is still
under active debate. We'll use unit tests where we can as hypotheses which pass
or fail, and aspect orientation allows us to detail our experiments as a series
of aspect calls representing a change to the expression of a problem domain. 


% ==============================================================================
\section{Conclusion}
\label{sec:concluion}














\bibliography{lib}

\end{document}